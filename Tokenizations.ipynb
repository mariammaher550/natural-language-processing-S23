{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tweets Tokenization\n",
    "\n",
    "The goal of the assignment is to write a tweet tokenizer. The input of the code will be a set of tweet text and the output will be the tokens in each tweet. The assignment is made up of four tasks.\n",
    "\n",
    "The [data](https://drive.google.com/file/d/15x_wPAflvYQ2Xh38iNQGrqUIWLj5l5Nw/view?usp=share_link) contains 5 files whereby each contains 44 tweets. Each tweet is separated by a newline. For manual tokenization only one file should be used.\n",
    "\n",
    "Grading:\n",
    "- 30 points - Tokenize tweets by hand\n",
    "- 30 points - Implement 4 tokenizers\n",
    "- 20 points - Stemming and Lemmatization\n",
    "- 20 points - Explain sentencepiece (for masters only)\n",
    "\n",
    "\n",
    "Remarks: \n",
    "- Use Python 3 or greater\n",
    "- Max is 80 points for bachelors, 100 points for masters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize tweets by hand\n",
    "\n",
    "As a first task you need to tokenize 15 tweets by hand. This will allow you to understand the problem from a linguistic point of view. The guidelines for tweet tokenization are as follows:\n",
    "\n",
    "- Each smiley is a separate token\n",
    "- Each hashtag is an individual token. Each user reference is an individual token\n",
    "- If a word has spaces between them then it is converted to a single token\n",
    "- If a sentence ends with a word that legitimately has a full stop (abbreviations, for example), add a final full stop\n",
    "- All punctuations are individual tokens. This includes double-quotes and single quotes also\n",
    "- A URL is a single token\n",
    "\n",
    "Example of output\n",
    "\n",
    "    Input tweet\n",
    "    @xfranman Old age has made N A T O!\n",
    "\n",
    "    Tokenized tweet (separated by comma)\n",
    "    @xfranman , Old , age , has , made , NATO , !"
   ],
   "metadata": {
    "id": "aLDjjAvemUP_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "    1. Input tweet\n",
    "    ...\n",
    "    1. Tokenized tweet\n",
    "    ...\n",
    "\n",
    "    2. Input tweet\n",
    "    ...\n",
    "    2. Tokenized tweet\n",
    "    ..."
   ],
   "metadata": {
    "id": "7KKKwTidnzUw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tweet #1:\n",
      " @anitapuspasari waduh..\n",
      "\n",
      "Tokenized tweet #1:\n",
      " \n",
      "@anitapuspasari , waduh.. , \n",
      "Input tweet #2:\n",
      " \" Could journos please stop putting the word \"\"gate\"\" after everything they write... gate.\"\n",
      "\n",
      "Tokenized tweet #2:\n",
      " \n",
      "\" ,  , Could , journos , please , stop , putting , the , word , \" , \" , gate , \" , \" ,  , after , everything , they , write... , gate. , \" ,  , \n",
      "Input tweet #3:\n",
      " 20% More Ridiculous Sale @20x200 ends tonight! - get 20% off by entering 'RIDONK' at checkout. More info: http://bit.ly/ridonktues\n",
      "\n",
      "Tokenized tweet #3:\n",
      " \n",
      "20% , More , Ridiculous , Sale , @20x200 , ends , tonight , ! ,  , - ,  , get , 20% , off , by , entering , ' , RIDONK , ' ,  , at , checkout. , More , info , : ,  , http://bit.ly/ridonktues , \n",
      "Input tweet #4:\n",
      " @Studio85 I have a pair of those shoes. They are comfy. Like being barefoot. Okay for running, but not on concrete, as I've discovered.\n",
      "\n",
      "Tokenized tweet #4:\n",
      " \n",
      "@Studio85 , I , have , a , pair , of , those , shoes. , They , are , comfy. , Like , being , barefoot. , Okay , for , running , , ,  , but , not , on , concrete , , ,  , as , I , ' , ve , discovered. , \n",
      "Input tweet #5:\n",
      " RT @twilightus Team Carlisle is a Trending Topic- help him out RT Follow @peterfacinelli see a grown man n a bikini dance Hollywood Blvd\n",
      "\n",
      "Tokenized tweet #5:\n",
      " \n",
      "RT , @twilightus , Team , Carlisle , is , a , Trending , Topic , - ,  , help , him , out , RT , Follow , @peterfacinelli , see , a , grown , man , na , bikini , dance , Hollywood , Blvd , \n",
      "Input tweet #6:\n",
      " @karenrubin you might have to reinstall - that happened to me a few months ago, now I use Nambu on my Mac\n",
      "\n",
      "Tokenized tweet #6:\n",
      " \n",
      "@karenrubin , you , might , have , to , reinstall , - ,  , that , happened , to , me , a , few , months , ago , , ,  , now , I , use , Nambu , on , my , Mac , \n",
      "Input tweet #7:\n",
      " Just Posted: Redneck Dragon - Part XXVIII (http://cli.gs/gWy0yT)\n",
      "\n",
      "Tokenized tweet #7:\n",
      " \n",
      "Just , Posted , : ,  , Redneck , Dragon , - ,  , Part , XXVIII , (http://cli.gs/gWy0yT) , \n",
      "Input tweet #8:\n",
      " \" \"\"Paul McCartney ... went through all his education there and nobody thought he had any musical talent,\"\" http://tinyurl.com/nkdbdq\"\n",
      "\n",
      "Tokenized tweet #8:\n",
      " \n",
      "\" ,  , \" , \" , Paul , McCartney , ... , went , through , all , his , education , there , and , nobody , thought , he , had , any , musical , talent , , , \" , \" ,  , http://tinyurl.com/nkdbdq\" , \n",
      "Input tweet #9:\n",
      " @ambienteer Yeah, pretty much how i feel about it.\n",
      "\n",
      "Tokenized tweet #9:\n",
      " \n",
      "@ambienteer , Yeah , , ,  , pretty , much , how , i , feel , about , it. , \n",
      "Input tweet #10:\n",
      " @florianseroussi Nothing really noticeable? Are you kidding?\n",
      "\n",
      "Tokenized tweet #10:\n",
      " \n",
      "@florianseroussi , Nothing , really , noticeable , ? ,  , Are , you , kidding , ? ,  , \n",
      "Input tweet #11:\n",
      " @toiletooth Hours?\n",
      "\n",
      "Tokenized tweet #11:\n",
      " \n",
      "@toiletooth , Hours , ? ,  , \n",
      "Input tweet #12:\n",
      " \" Obama,Hamas,and the Mullahs being \"\"helpfu l\"\"http://www.jpost.com/servlet/Satellite?cid=1245184848467&pagename=JPost%2FJPArticle%2FPrinter\"\n",
      "\n",
      "Tokenized tweet #12:\n",
      " \n",
      "\" ,  , Obama , , , Hamas , , , and , the , Mullahs , being , \" , \" , helpfu , l\"\"http://www.jpost.com/servlet/Satellite?cid=1245184848467&pagename=JPost%2FJPArticle%2FPrinter\" , \n",
      "Input tweet #13:\n",
      " RT @BBHLabs 81% of twitter users are UNDER 30 + more v. interesting statistics here: http://www.sysomos.com/insidetwitter/\n",
      "\n",
      "Tokenized tweet #13:\n",
      " \n",
      "RT , @BBHLabs , 81% , of , twitter , users , are , UNDER , 30 , + , more , v. , interesting , statistics , here , : ,  , http://www.sysomos.com/insidetwitter/ , \n",
      "Input tweet #14:\n",
      " @Birdingperu Great looking hummer! RTThe world's most spectacular hummingbird Marvelous Spatuletail on a feeder. http://bit.ly/aGHYZ\n",
      "\n",
      "Tokenized tweet #14:\n",
      " \n",
      "@Birdingperu , Great , looking , hummer , ! ,  , RTThe , world , ' , s , most , spectacular , hummingbird , Marvelous , Spatuletail , on , a , feeder. , http://bit.ly/aGHYZ , \n",
      "Input tweet #15:\n",
      " attn. chas. whitman: RT @villagevoice Jonas Brothers at Rockefeller Center for the Today Show tomorrow mornâ€”EEEEEEEEEE!\n",
      "\n",
      "Tokenized tweet #15:\n",
      " \n",
      "attn. , chas. , whitman , : ,  , RT , @villagevoice , Jonas , Brothers , at , Rockefeller , Center , for , the , Today , Show , tomorrow , mornâ€”EEEEEEEEEE , ! ,  , \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# since we need to tokenize by hand, so I didn't use regex nor nltk\n",
    "\n",
    "def is_url(text):\n",
    "\n",
    "    if \"http\" in text:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    punct = [\"'\", '\"', \",\", \":\", \";\", \"?\", \"!\", \"-\", \"~\", \"/\"]\n",
    "    tokens = []\n",
    "    tok = \"\"\n",
    "\n",
    "    # TODO : seperate punct as indivdual tokens\n",
    "    # TODO : emojis as single token\n",
    "\n",
    "    for word in text.split():\n",
    "\n",
    "        # checking if there is a word with spaces inbetween\n",
    "        if len(word) == 1 and word not in punct:\n",
    "            tok += word\n",
    "            continue\n",
    "\n",
    "        if tok:\n",
    "            tokens.append(tok)\n",
    "            tok = \"\"\n",
    "\n",
    "        if is_url(word):\n",
    "            tokens.append(word)\n",
    "            continue\n",
    "        # removing punctutations in words as a single token\n",
    "        word_part=\"\"\n",
    "        for c in word:\n",
    "            if c in punct:\n",
    "                if word_part:\n",
    "                    tokens.append(word_part)\n",
    "                    word_part = \"\"\n",
    "                tokens.append(c)\n",
    "                continue\n",
    "            word_part += c\n",
    "\n",
    "        tokens.append(word_part)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "i = 1\n",
    "for tweet in open(\"data/assignment1/file1\", 'r'):\n",
    "    print(f\"Input tweet #{i}:\\n {tweet}\")\n",
    "    print(f\"Tokenized tweet #{i}:\\n \")\n",
    "    token_str = \"\"\n",
    "    for token in tokenize(tweet):\n",
    "        token_str += token + ' , '\n",
    "    print(token_str)\n",
    "    if i >= 15:\n",
    "        break\n",
    "    i+=1\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implement 4 tokenizers\n",
    "\n",
    "Your task is to implement the 4 different tokenizers that take a list of tweets on a topic and output tokenization for each:\n",
    "\n",
    "- White Space Tokenization\n",
    "- Sentencepiece\n",
    "- Tokenizing text using regular expressions\n",
    "- NLTK TweetTokenizer\n",
    "\n",
    "For tokenizing text using regular expressions use the rules in task 1. Combine task 1 rules into regular expression and create a tokenizer."
   ],
   "metadata": {
    "id": "-2J2AD2nmUhi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def white_space_tokenizer(text: str):\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ],
   "metadata": {
    "id": "4uZId1tjrfyz"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# based on : https://github.com/google/sentencepiece/blob/master/python/README.md\n",
    "# based on https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "def prepare_data_file(files,dataset):\n",
    "    data=\"\"\n",
    "    for i in files:\n",
    "        with open (f\"data/assignment1/file{i}\", 'r', encoding=\"utf8\") as file:\n",
    "            data += file.read()\n",
    "            data += \"\\n\"\n",
    "\n",
    "    with open(f\"{dataset}.txt\", \"w\", encoding=\"utf8\") as file:\n",
    "        file.write(data)\n",
    "\n",
    "\n",
    "# Here I used the first 3 data files as training data and\n",
    "# the last two as testing data\n",
    "\n",
    "prepare_data_file([\"1\", \"2\", \"3\"], \"train\")\n",
    "prepare_data_file([\"4\", \"5\"], \"test\")\n",
    "\n",
    "# training sentencePiece trainer\n",
    "# the whole vocabulary list from the training data is ~1300, seems appropriate size to use\n",
    "\n",
    "spm.SentencePieceTrainer.train('--input=train.txt --model_prefix=m --vocab_size=1300')\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "def sentencepiece_wrapper(text: str):\n",
    "    tokens = sp.Encode(text, out_type= str)\n",
    "    return tokens\n",
    "\n",
    "test_data =\"\"\n",
    "with open (\"test.txt\", 'r', encoding=\"utf8\") as file:\n",
    "            test_data += file.read()\n",
    "\n",
    "# for line in test_data.split(\"\\n\"):\n",
    "#     print(sentencepiece_wrapper(line))"
   ],
   "metadata": {
    "id": "4ou1WE8Krf_W"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "def re_tokenizer(text: str):\n",
    "    tokens=[]\n",
    "    for word in text.split():\n",
    "        tokens += re.findall(r'(https?://[^\\s]+)|(\\B#\\w+\\b|@\\w+\\b)|(\\b\\w+\\b|[^\\s]+)|(\\W+)', word)\n",
    "\n",
    "    res = []\n",
    "    for token  in tokens:\n",
    "        for word in token:\n",
    "            if word != '':\n",
    "                res.append(word)\n",
    "\n",
    "    return res"
   ],
   "metadata": {
    "id": "pC32dK_urf5P"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# based on https://www.geeksforgeeks.org/python-nltk-nltk-tweettokenizer/\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def nltk_tweet_tokenizer(text: str) :\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens"
   ],
   "metadata": {
    "id": "Q8UVniWVrgMT"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run your implementations on the data. Compare the results, decide which one is better. List the advantages of the best tokenizer."
   ],
   "metadata": {
    "id": "yIhPteb4s_Yn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying TweeTokenizer\n",
      "\n",
      "this tokenizer took 0.03604841232299805 to tokenize 91 tweets\n",
      " Here is an example \n",
      " Input tweet: \n",
      " @marklobbezoo Mooie winnaar!\n",
      " Tokenized tweet: \n",
      " ['@marklobbezoo', 'Mooie', 'winnaar', '!']\n",
      "------------------------\n",
      "\n",
      "Trying re_tokenizer\n",
      "\n",
      "this tokenizer took 0.008000612258911133 to tokenize 91 tweets\n",
      " Here is an example \n",
      " Input tweet: \n",
      " @marklobbezoo Mooie winnaar!\n",
      " Tokenized tweet: \n",
      " ['@marklobbezoo', 'Mooie', 'winnaar', '!']\n",
      "------------------------\n",
      "\n",
      "Trying whitespace tokenizer\n",
      "\n",
      "this tokenizer took 0.0010025501251220703 to tokenize 91 tweets\n",
      " Here is an example \n",
      " Input tweet: \n",
      " @marklobbezoo Mooie winnaar!\n",
      " Tokenized tweet: \n",
      " ['@marklobbezoo', 'Mooie', 'winnaar!']\n",
      "------------------------\n",
      "\n",
      "Trying sentiencePiece\n",
      "\n",
      "this tokenizer took 0.016391277313232422 to tokenize 91 tweets\n",
      " Here is an example \n",
      " Input tweet: \n",
      " @marklobbezoo Mooie winnaar!\n",
      " Tokenized tweet: \n",
      " ['▁@', 'm', 'ark', 'lo', 'bb', 'ez', 'oo', '▁Mo', 'o', 'ie', '▁wi', 'n', 'n', 'aar', '!']\n",
      "------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run tokenizer on the data\n",
    "# TODO: Time the tokenization process\n",
    "\n",
    "import time\n",
    "def run_tokenizer(tokenizer):\n",
    "    st = time.time()\n",
    "    num_tweets = 0\n",
    "    tokens=[]\n",
    "    for tweet in test_data.split('\\n'):\n",
    "        tokens += tokenizer(tweet)\n",
    "        num_tweets +=1\n",
    "    et = time.time()\n",
    "    elapsed_time = et-st\n",
    "\n",
    "    # printing states\n",
    "    example_tweet = test_data.split('\\n')[0]\n",
    "    print(f\"this tokenizer took {elapsed_time} to tokenize {num_tweets} tweets\\n Here is an example \\n Input tweet: \\n {example_tweet}\\n Tokenized tweet: \\n {tokenizer(example_tweet)}\")\n",
    "\n",
    "\n",
    "print(\"Trying TweeTokenizer\\n\")\n",
    "run_tokenizer(nltk_tweet_tokenizer)\n",
    "print(\"------------------------\\n\")\n",
    "\n",
    "print(\"Trying re_tokenizer\\n\")\n",
    "run_tokenizer(re_tokenizer)\n",
    "print(\"------------------------\\n\")\n",
    "\n",
    "print(\"Trying whitespace tokenizer\\n\")\n",
    "run_tokenizer(white_space_tokenizer)\n",
    "print(\"------------------------\\n\")\n",
    "\n",
    "print(\"Trying sentiencePiece\\n\")\n",
    "run_tokenizer(sentencepiece_wrapper)\n",
    "print(\"------------------------\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After trying all the tokenizers on same tweets. I have noticed that the fastest is whitespace tokenizer and the slowest is tweettokenizer. I don't believe there is an ultimate \"best\" tokenizer as it relative to each task/domain. For this specific task I will pick tweettokenizer since it is modeled specifically for the problem we have at hand (tokenizing tweets). But if the domain was more ambiguous then I will go with sentencepeice since it tokenize based on the dataset it as at hand - as I have aforementioned in \"Explain Sentencepeice\" section."
   ],
   "metadata": {
    "id": "muAZeHkMtaCd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Your task is to write two functions: stem and lemmatize. Input is a text, so you need to tokenize it first."
   ],
   "metadata": {
    "id": "MlzpzjgEpY-R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# based on https://www.geeksforgeeks.org/snowball-stemmer-nlp/\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def stem(text: str):\n",
    "    # I will just use tweettokenizer\n",
    "    tokens = nltk_tweet_tokenizer(text)\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "    stem_words=[]\n",
    "    for w in tokens:\n",
    "        x = stemmer.stem(w)\n",
    "        stem_words.append(x)\n",
    "\n",
    "    return stem_words"
   ],
   "metadata": {
    "id": "ghAR1rSjsnz1"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# based on https://www.projectpro.io/recipes/use-spacy-lemmatizer\n",
    "import spacy\n",
    "\n",
    "# kept the tagger but disabled the parser and ner as they are not needed\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatize(text: str) :\n",
    "    tokens = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in tokens]\n",
    "    return lemmas"
   ],
   "metadata": {
    "id": "kZkR3TPYuk5T"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Explain sentencepiece (for masters only)\n",
    "\n",
    "For this task you will have to use sentencepiece text tokenizer. Your task will be to read how it works and write a minimum 10 sentences explanation of the tokenizer works."
   ],
   "metadata": {
    "id": "islrHZ6UmUoh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# SentencePiece Concept\n",
    "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing. It treats the text as sequence of unicode charachters thus making it a\n",
    "\n",
    "\n",
    "# SentencePiece components\n",
    "Sentencepiece is composed of four components , normaliser, encoder, decoder and a trainer as shown bellow\n",
    "![](../../AppData/Local/Temp/0_wbfbmY9bT_3fbWq7.png)\n",
    "\n",
    "normalizer : It standerdize the words of the text to a specific format. according to its docs https://github.com/google/sentencepiece/blob/master/doc/normalization.md it follows part of NFKC normalization. It also includes the option to use custom normalization rules\n",
    "encoder/decoder: They are used to map back and forth. The equation from SentencePiece paper  as known as lossless tokenization , is Decode(Encode(Normalized(text)) = Normalized(text). It handles whitespaces by substituting it with underscores \"_\" and decode by substituting the underscores back with whitespaces. T\n",
    "trainer : it uses either Byte-Pair Encoding or Unigram model to built up a world vocabulary based on sub-word components\n",
    "\n",
    "# What if there are multiple ways to split up the word based on the vocabulary list?\n",
    "\n",
    "Another way to phrase the problem is \"How do we best split a sentence to ensure that when words are used in the same context, they are matched to the same IDs?\"\n",
    "\n",
    "SentencePiece aims to solve this problem by using Subword regularization in which the segemenation model consider different ways to split the sentence, thus making the neural model accurate and robust."
   ],
   "metadata": {
    "id": "e2RjMwlEshCu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Resources\n",
    "\n",
    "1. [Regular Expressions 1](https://realpython.com/regex-python/)\n",
    "2. [Regular Expressions 2](https://realpython.com/regex-python-part-2/)\n",
    "2. [Spacy Lemmatizer](https://spacy.io/api/lemmatizer)\n",
    "2. [NLTK Stem](https://www.nltk.org/howto/stem.html)\n",
    "3. [SentencePiece](https://github.com/google/sentencepiece)\n",
    "4. [sentencepiece tokenizer](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)"
   ],
   "metadata": {
    "id": "cmNpWzfLmUyE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 3.8 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install libraries\n",
    "!pip install sentencepiece\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
